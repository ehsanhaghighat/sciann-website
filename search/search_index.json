{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SciANN: Neural Networks for Scientific Computations New to SciANN? SciANN is a high-level artificial neural networks API, written in Python using Keras and TensorFlow backends. It is developed with a focus on enabling fast experimentation with different networks architectures and with emphasis on scientific computations, physics informed deep learing, and inversion. Being able to start deep-learning in a very few lines of code is key to doing good research. Use SciANN if you need a deep learning library that: Allows for easy and fast prototyping. Allows the use of complex deep neural networks. Takes advantage TensorFlow and Keras features including seamlessly running on CPU and GPU. For more details, check out our review paper at https://arxiv.org/abs/2005.08803 and the documentation at SciANN.com . Have questions or would like to contribute, please join sciann.slack.com . Cite SciANN in your publications if it helps your research: @article{haghighat2020sciann, title = {SciANN: A Keras/TensorFlow wrapper for scientific computations and physics-informed deep learning using artificial neural networks}, journal = {Computer Methods in Applied Mechanics and Engineering}, volume = {373}, pages = {113552}, year = {2021}, issn = {0045-7825}, doi = {https://doi.org/10.1016/j.cma.2020.113552}, url = {https://www.sciencedirect.com/science/article/pii/S0045782520307374}, author = {Ehsan Haghighat and Ruben Juanes}, keywords = {SciANN, Deep neural networks, Scientific computations, PINN, vPINN}, } SciANN is compatible with: Python 2.7-3.6 . You can also email me at Ehsan Haghighat . Getting started: 30 seconds to SciANN The core data structure of SciANN is a Functional , a way to organize inputs ( Variables ) and outputs ( Fields ) of a network. Targets are imposed on Functional instances using Constraint s. The SciANN model ( SciModel ) is formed from inputs ( Variables ) and targets( Constraints ). The model is then trained by calling the solve function. Here is the simplest SciANN model: from sciann import Variable, Functional, SciModel from sciann.constraints import Data x = Variable('x') y = Functional('y') # y_true is a Numpy array of (N,1) -- with N as number of samples. model = SciModel(x, Data(y)) This is associated to the simplest neural network possible, i.e. a linear relation between the input variable x and the output variable y with only two parameters to be learned. Plotting a network is as easy as passing a file_name to the SciModel: model = SciModel(x, Data(y), plot_to_file='file_path') Once your model looks good, perform the learning with .solve() : # x_true is a Numpy array of (N,1) -- with N as number of samples. model.train(x_true, y_true, epochs=5, batch_size=32) You can iterate on your training data in batches and in multiple epochs. Please check Keras documentation on model.fit for more information on possible options. You can evaluate the model any time on new data: classes = model.predict(x_test, batch_size=128) In the application folder of the repository, you will find some examples of Linear Elasticity, Flow, Flow in Porous Media, etc. Installation Before installing SciANN, you need to install the TensorFlow and Keras. TensorFlow installation instructions . Keras installation instructions . You may also consider installing the following optional dependencies : cuDNN (recommended if you plan on running Keras on GPU). HDF5 and h5py (required if you plan on saving Keras/SciANN models to disk). graphviz and pydot (used by visualization utilities to plot model graphs). Then, you can install SciANN itself. There are two ways to install SciANN: Install SciANN from PyPI (recommended): Note: These installation steps assume that you are on a Linux or Mac environment. If you are on Windows, you will need to remove sudo to run the commands below. sudo pip install sciann If you are using a virtualenv, you may want to avoid using sudo: pip install sciann Alternatively: install SciANN from the GitHub source: First, clone SciANN using git : git clone https://github.com/sciann/sciann.git Then, cd to the SciANN folder and run the install command: sudo python setup.py install or sudo pip install . Why this name, SciANN? Scientific Computational with Artificial Neural Networks. Scientific computations include solving ODEs, PDEs, Integration, Differentiation, Curve Fitting, etc.","title":"Home"},{"location":"#sciann-neural-networks-for-scientific-computations","text":"","title":"SciANN: Neural Networks for Scientific Computations"},{"location":"#new-to-sciann","text":"SciANN is a high-level artificial neural networks API, written in Python using Keras and TensorFlow backends. It is developed with a focus on enabling fast experimentation with different networks architectures and with emphasis on scientific computations, physics informed deep learing, and inversion. Being able to start deep-learning in a very few lines of code is key to doing good research. Use SciANN if you need a deep learning library that: Allows for easy and fast prototyping. Allows the use of complex deep neural networks. Takes advantage TensorFlow and Keras features including seamlessly running on CPU and GPU. For more details, check out our review paper at https://arxiv.org/abs/2005.08803 and the documentation at SciANN.com . Have questions or would like to contribute, please join sciann.slack.com . Cite SciANN in your publications if it helps your research: @article{haghighat2020sciann, title = {SciANN: A Keras/TensorFlow wrapper for scientific computations and physics-informed deep learning using artificial neural networks}, journal = {Computer Methods in Applied Mechanics and Engineering}, volume = {373}, pages = {113552}, year = {2021}, issn = {0045-7825}, doi = {https://doi.org/10.1016/j.cma.2020.113552}, url = {https://www.sciencedirect.com/science/article/pii/S0045782520307374}, author = {Ehsan Haghighat and Ruben Juanes}, keywords = {SciANN, Deep neural networks, Scientific computations, PINN, vPINN}, } SciANN is compatible with: Python 2.7-3.6 . You can also email me at Ehsan Haghighat .","title":"New to SciANN?"},{"location":"#getting-started-30-seconds-to-sciann","text":"The core data structure of SciANN is a Functional , a way to organize inputs ( Variables ) and outputs ( Fields ) of a network. Targets are imposed on Functional instances using Constraint s. The SciANN model ( SciModel ) is formed from inputs ( Variables ) and targets( Constraints ). The model is then trained by calling the solve function. Here is the simplest SciANN model: from sciann import Variable, Functional, SciModel from sciann.constraints import Data x = Variable('x') y = Functional('y') # y_true is a Numpy array of (N,1) -- with N as number of samples. model = SciModel(x, Data(y)) This is associated to the simplest neural network possible, i.e. a linear relation between the input variable x and the output variable y with only two parameters to be learned. Plotting a network is as easy as passing a file_name to the SciModel: model = SciModel(x, Data(y), plot_to_file='file_path') Once your model looks good, perform the learning with .solve() : # x_true is a Numpy array of (N,1) -- with N as number of samples. model.train(x_true, y_true, epochs=5, batch_size=32) You can iterate on your training data in batches and in multiple epochs. Please check Keras documentation on model.fit for more information on possible options. You can evaluate the model any time on new data: classes = model.predict(x_test, batch_size=128) In the application folder of the repository, you will find some examples of Linear Elasticity, Flow, Flow in Porous Media, etc.","title":"Getting started: 30 seconds to SciANN"},{"location":"#installation","text":"Before installing SciANN, you need to install the TensorFlow and Keras. TensorFlow installation instructions . Keras installation instructions . You may also consider installing the following optional dependencies : cuDNN (recommended if you plan on running Keras on GPU). HDF5 and h5py (required if you plan on saving Keras/SciANN models to disk). graphviz and pydot (used by visualization utilities to plot model graphs). Then, you can install SciANN itself. There are two ways to install SciANN: Install SciANN from PyPI (recommended): Note: These installation steps assume that you are on a Linux or Mac environment. If you are on Windows, you will need to remove sudo to run the commands below. sudo pip install sciann If you are using a virtualenv, you may want to avoid using sudo: pip install sciann Alternatively: install SciANN from the GitHub source: First, clone SciANN using git : git clone https://github.com/sciann/sciann.git Then, cd to the SciANN folder and run the install command: sudo python setup.py install or sudo pip install .","title":"Installation"},{"location":"#why-this-name-sciann","text":"Scientific Computational with Artificial Neural Networks. Scientific computations include solving ODEs, PDEs, Integration, Differentiation, Curve Fitting, etc.","title":"Why this name, SciANN?"},{"location":"constraints/","text":"Intro Constraint contains set of classes to impose conditions on the targets or their derivatives. This classes are designed as a way to impose constraints on different parts of targets and domain. [source] Data sciann.constraints.data.Data(cond, name='data') Data class to impose to the system. Arguments cond : Functional. The Functional object that Data condition will be imposed on. name : String. A str for name of the pde. Returns Raises ValueError : 'cond' should be a functional object. 'mesh' should be a list of numpy arrays. [source] PDE sciann.constraints.pde.PDE(pde, name='pde') PDE class to impose to the system. Arguments pde : Functional. The Functional object that pde if formed on. name : String. A str for name of the pde. Returns Raises ValueError : 'pde' should be a functional object. [source] Tie sciann.constraints.tie.Tie(cond1, cond2, name='tie') Tie class to constrain network outputs. constraint: cond1 - cond2 == sol . Arguments cond1 : Functional. A Functional object to be tied to cond2. cond2 : Functional. A 'Functional' object to be tied to cond1. name : String. A str for name of the pde. Returns Raises ValueError : 'pde' should be a functional object.","title":"Constraints"},{"location":"constraints/#intro","text":"Constraint contains set of classes to impose conditions on the targets or their derivatives. This classes are designed as a way to impose constraints on different parts of targets and domain. [source]","title":"Intro"},{"location":"constraints/#data","text":"sciann.constraints.data.Data(cond, name='data') Data class to impose to the system. Arguments cond : Functional. The Functional object that Data condition will be imposed on. name : String. A str for name of the pde. Returns Raises ValueError : 'cond' should be a functional object. 'mesh' should be a list of numpy arrays. [source]","title":"Data"},{"location":"constraints/#pde","text":"sciann.constraints.pde.PDE(pde, name='pde') PDE class to impose to the system. Arguments pde : Functional. The Functional object that pde if formed on. name : String. A str for name of the pde. Returns Raises ValueError : 'pde' should be a functional object. [source]","title":"PDE"},{"location":"constraints/#tie","text":"sciann.constraints.tie.Tie(cond1, cond2, name='tie') Tie class to constrain network outputs. constraint: cond1 - cond2 == sol . Arguments cond1 : Functional. A Functional object to be tied to cond2. cond2 : Functional. A 'Functional' object to be tied to cond1. name : String. A str for name of the pde. Returns Raises ValueError : 'pde' should be a functional object.","title":"Tie"},{"location":"contributing/","text":"On Github Issues and Pull Requests Found a bug? Have a new feature to suggest? Want to contribute changes to the codebase? Make sure to read this first. Adding new examples Even if you don't contribute to the Keras source code, if you have an application of Keras that is concise and powerful, please consider adding it to our collection of examples: Existing examples .","title":"On Github Issues and Pull Requests"},{"location":"contributing/#on-github-issues-and-pull-requests","text":"Found a bug? Have a new feature to suggest? Want to contribute changes to the codebase? Make sure to read this first.","title":"On Github Issues and Pull Requests"},{"location":"contributing/#adding-new-examples","text":"Even if you don't contribute to the Keras source code, if you have an application of Keras that is concise and powerful, please consider adding it to our collection of examples: Existing examples .","title":"Adding new examples"},{"location":"examples/","text":"Examples To understand the design philosophy behind SciANN, please check our review paper: https://arxiv.org/abs/2005.08803 A great repository of examples is being created on SciANN's github repository: SciANN-applications A simple example can be found in sciann installation folder: example-fitting-1D Simple linear and quadratic regression: SciANN-Regression.ipynb Curve-fitting using neural networks: SciANN-NN-Regression.ipynb Application to linear-elasticity: SciANN-SolidMechanics {{autogenerated}}","title":"Examples"},{"location":"examples/#examples","text":"To understand the design philosophy behind SciANN, please check our review paper: https://arxiv.org/abs/2005.08803 A great repository of examples is being created on SciANN's github repository: SciANN-applications A simple example can be found in sciann installation folder: example-fitting-1D Simple linear and quadratic regression: SciANN-Regression.ipynb Curve-fitting using neural networks: SciANN-NN-Regression.ipynb Application to linear-elasticity: SciANN-SolidMechanics {{autogenerated}}","title":"Examples"},{"location":"fields/","text":"Intro Field is a layer to define outputs of each Functional. It is very much similar to Keras' Dense layer. It is not necessary to be defined explicitly, however, if you are expecting multiple outputs, it is better to be defined using Field . from sciann import Field Fx = Field(name='Fx', units=10) [source] Field sciann.functionals.field.Field(name=None, units=1, activation=<function linear at 0x7fbb9b26d050>, kernel_initializer=<tensorflow.python.ops.init_ops.GlorotNormal object at 0x7fbb9bc47190>, bias_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomUniform object at 0x7fbb9bc471d0>, kernel_regularizer=None, bias_regularizer=None, trainable=True, dtype=None) Configures the Field class for the model outputs. Arguments name : String. Assigns a layer name for the output. units : Positive integer. Dimension of the output of the network. activation : Callable. A callable object for the activation. kernel_initializer : Initializer for the kernel. Defaulted to a normal distribution. bias_initializer : Initializer for the bias. Defaulted to a normal distribution. kernel_regularizer : Regularizer for the kernel. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. bias_regularizer : Regularizer for the bias. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. trainable : Boolean to activate parameters of the network. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises","title":"Fields"},{"location":"fields/#intro","text":"Field is a layer to define outputs of each Functional. It is very much similar to Keras' Dense layer. It is not necessary to be defined explicitly, however, if you are expecting multiple outputs, it is better to be defined using Field . from sciann import Field Fx = Field(name='Fx', units=10) [source]","title":"Intro"},{"location":"fields/#field","text":"sciann.functionals.field.Field(name=None, units=1, activation=<function linear at 0x7fbb9b26d050>, kernel_initializer=<tensorflow.python.ops.init_ops.GlorotNormal object at 0x7fbb9bc47190>, bias_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomUniform object at 0x7fbb9bc471d0>, kernel_regularizer=None, bias_regularizer=None, trainable=True, dtype=None) Configures the Field class for the model outputs. Arguments name : String. Assigns a layer name for the output. units : Positive integer. Dimension of the output of the network. activation : Callable. A callable object for the activation. kernel_initializer : Initializer for the kernel. Defaulted to a normal distribution. bias_initializer : Initializer for the bias. Defaulted to a normal distribution. kernel_regularizer : Regularizer for the kernel. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. bias_regularizer : Regularizer for the bias. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. trainable : Boolean to activate parameters of the network. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises","title":"Field"},{"location":"functionals/","text":"Intro A combination of neural network layers form a Functional . Mathematically, a functional is a general mapping from input set \\(X\\) onto some output set \\(Y\\). Once the parameters of this transformation are found, this mapping is called a function . Functional s are needed to form SciModels . A Functional is a class to form complex architectures (mappings) from inputs ( Variables ) to the outputs. from sciann import Variable, Functional x = Variable('x') y = Variable('y') Fxy = Functional('Fxy', [x, y], hidden_layers=[10, 20, 10], activation='tanh') Functionals can be plotted when a SciModel is formed. A minimum of one Constraint is needed to form the SciModel from sciann.constraints import Data from sciann import SciModel model = SciModel(x, Data(Fxy), plot_to_file='output.png') [source] Functional sciann.functionals.functional.Functional(fields=None, variables=None, hidden_layers=None, activation='tanh', output_activation='linear', res_net=False, kernel_initializer=None, bias_initializer=None, kernel_regularizer=None, bias_regularizer=None, dtype=None, trainable=True) Configures the Functional object (Neural Network). Arguments fields : String or Field. [Sub-]Network outputs. It can be of type String - Associated fields will be created internally. It can be of type Field or Functional variables : Variable. [Sub-]Network inputs. It can be of type Variable or other Functional objects. hidden_layers : A list indicating neurons in the hidden layers. e.g. [10, 100, 20] is a for hidden layers with 10, 100, 20, respectively. activation : defaulted to \"tanh\". Activation function for the hidden layers. Last layer will have a linear output. output_activation : defaulted to \"linear\". Activation function to be applied to the network output. res_net : (True, False). Constructs a resnet architecture. Defaulted to False. kernel_initializer : Initializer of the Kernel , from k.initializers . bias_initializer : Initializer of the Bias , from k.initializers . kernel_regularizer : Regularizer for the kernel. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. bias_regularizer : Regularizer for the bias. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Note: Only network inputs should be set. trainable : Boolean. False if network is not trainable, True otherwise. Default value is True. Raises ValueError : TypeError : [source] Variable sciann.functionals.variable.Variable(name=None, units=1, tensor=None, dtype=None) Configures the Variable object for the network's input. Arguments name : String. Required as derivatives work only with layer names. units : Int. Number of feature of input var. tensor : Tensorflow Tensor . Can be pass as the input path. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises [source] Field sciann.functionals.field.Field(name=None, units=1, activation=<function linear at 0x7fbb9b26d050>, kernel_initializer=<tensorflow.python.ops.init_ops.GlorotNormal object at 0x7fbb9bc47190>, bias_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomUniform object at 0x7fbb9bc471d0>, kernel_regularizer=None, bias_regularizer=None, trainable=True, dtype=None) Configures the Field class for the model outputs. Arguments name : String. Assigns a layer name for the output. units : Positive integer. Dimension of the output of the network. activation : Callable. A callable object for the activation. kernel_initializer : Initializer for the kernel. Defaulted to a normal distribution. bias_initializer : Initializer for the bias. Defaulted to a normal distribution. kernel_regularizer : Regularizer for the kernel. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. bias_regularizer : Regularizer for the bias. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. trainable : Boolean to activate parameters of the network. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises [source] Parameter sciann.functionals.parameter.Parameter(val=1.0, min_max=None, inputs=None, name=None, non_neg=None) Parameter functional to be used for parameter inversion. Inherited from Dense layer. Arguments val : float. Initial value for the parameter. min_max : [MIN, MAX]. A range to constrain the value of parameter. This constraint will overwrite non_neg constraint if both are chosen. inputs : Variables. List of Variable s to the parameters. name : str. A name for the Parameter layer. non_neg : boolean. True (default) if only non-negative values are expected. **kwargs : keras.layer.Dense accepted arguments. eval eval() Evaluates the functional object for a given input. Arguments (SciModel, Xs): Evalutes the functional object from the beginning of the graph defined with SciModel. The Xs should match those of SciModel. (Xs): Evaluates the functional object from inputs of the functional. Xs should match those of inputs to the functional. Returns Numpy array of dimensions of network outputs. Raises ValueError : TypeError : get_weights get_weights(at_layer=None) Get the weights and biases of different layers. Arguments at_layer : Get the weights of a specific layer. Returns List of numpy array. set_weights set_weights(weights) Set the weights and biases of different layers. Arguments weights : Should take the dimensions as the output of \".get_weights\" Returns reinitialize_weights reinitialize_weights() Re-initialize the weights and biases of a functional object. Arguments Returns count_params count_params() Total number of parameters of a functional. Arguments Returns Total number of parameters. set_trainable set_trainable(val, layers=None) Set the weights and biases of a functional object trainable or not-trainable. Note: The SciModel should be called after this. Arguments val : (Ture, False) layers : list of layers to be set trainable or non-trainable. defaulted to None. Returns split split() In the case of Functional with multiple outputs, you can split the outputs and get an associated functional. Returns (f1, f2, ...): Tuple of splitted Functional objects associated to each output.","title":"Functionals"},{"location":"functionals/#intro","text":"A combination of neural network layers form a Functional . Mathematically, a functional is a general mapping from input set \\(X\\) onto some output set \\(Y\\). Once the parameters of this transformation are found, this mapping is called a function . Functional s are needed to form SciModels . A Functional is a class to form complex architectures (mappings) from inputs ( Variables ) to the outputs. from sciann import Variable, Functional x = Variable('x') y = Variable('y') Fxy = Functional('Fxy', [x, y], hidden_layers=[10, 20, 10], activation='tanh') Functionals can be plotted when a SciModel is formed. A minimum of one Constraint is needed to form the SciModel from sciann.constraints import Data from sciann import SciModel model = SciModel(x, Data(Fxy), plot_to_file='output.png') [source]","title":"Intro"},{"location":"functionals/#functional","text":"sciann.functionals.functional.Functional(fields=None, variables=None, hidden_layers=None, activation='tanh', output_activation='linear', res_net=False, kernel_initializer=None, bias_initializer=None, kernel_regularizer=None, bias_regularizer=None, dtype=None, trainable=True) Configures the Functional object (Neural Network). Arguments fields : String or Field. [Sub-]Network outputs. It can be of type String - Associated fields will be created internally. It can be of type Field or Functional variables : Variable. [Sub-]Network inputs. It can be of type Variable or other Functional objects. hidden_layers : A list indicating neurons in the hidden layers. e.g. [10, 100, 20] is a for hidden layers with 10, 100, 20, respectively. activation : defaulted to \"tanh\". Activation function for the hidden layers. Last layer will have a linear output. output_activation : defaulted to \"linear\". Activation function to be applied to the network output. res_net : (True, False). Constructs a resnet architecture. Defaulted to False. kernel_initializer : Initializer of the Kernel , from k.initializers . bias_initializer : Initializer of the Bias , from k.initializers . kernel_regularizer : Regularizer for the kernel. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. bias_regularizer : Regularizer for the bias. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Note: Only network inputs should be set. trainable : Boolean. False if network is not trainable, True otherwise. Default value is True. Raises ValueError : TypeError : [source]","title":"Functional"},{"location":"functionals/#variable","text":"sciann.functionals.variable.Variable(name=None, units=1, tensor=None, dtype=None) Configures the Variable object for the network's input. Arguments name : String. Required as derivatives work only with layer names. units : Int. Number of feature of input var. tensor : Tensorflow Tensor . Can be pass as the input path. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises [source]","title":"Variable"},{"location":"functionals/#field","text":"sciann.functionals.field.Field(name=None, units=1, activation=<function linear at 0x7fbb9b26d050>, kernel_initializer=<tensorflow.python.ops.init_ops.GlorotNormal object at 0x7fbb9bc47190>, bias_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomUniform object at 0x7fbb9bc471d0>, kernel_regularizer=None, bias_regularizer=None, trainable=True, dtype=None) Configures the Field class for the model outputs. Arguments name : String. Assigns a layer name for the output. units : Positive integer. Dimension of the output of the network. activation : Callable. A callable object for the activation. kernel_initializer : Initializer for the kernel. Defaulted to a normal distribution. bias_initializer : Initializer for the bias. Defaulted to a normal distribution. kernel_regularizer : Regularizer for the kernel. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. bias_regularizer : Regularizer for the bias. To set l1 and l2 to custom values, pass [l1, l2] or {'l1':l1, 'l2':l2}. trainable : Boolean to activate parameters of the network. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises [source]","title":"Field"},{"location":"functionals/#parameter","text":"sciann.functionals.parameter.Parameter(val=1.0, min_max=None, inputs=None, name=None, non_neg=None) Parameter functional to be used for parameter inversion. Inherited from Dense layer. Arguments val : float. Initial value for the parameter. min_max : [MIN, MAX]. A range to constrain the value of parameter. This constraint will overwrite non_neg constraint if both are chosen. inputs : Variables. List of Variable s to the parameters. name : str. A name for the Parameter layer. non_neg : boolean. True (default) if only non-negative values are expected. **kwargs : keras.layer.Dense accepted arguments.","title":"Parameter"},{"location":"functionals/#eval","text":"eval() Evaluates the functional object for a given input. Arguments (SciModel, Xs): Evalutes the functional object from the beginning of the graph defined with SciModel. The Xs should match those of SciModel. (Xs): Evaluates the functional object from inputs of the functional. Xs should match those of inputs to the functional. Returns Numpy array of dimensions of network outputs. Raises ValueError : TypeError :","title":"eval"},{"location":"functionals/#get_weights","text":"get_weights(at_layer=None) Get the weights and biases of different layers. Arguments at_layer : Get the weights of a specific layer. Returns List of numpy array.","title":"get_weights"},{"location":"functionals/#set_weights","text":"set_weights(weights) Set the weights and biases of different layers. Arguments weights : Should take the dimensions as the output of \".get_weights\" Returns","title":"set_weights"},{"location":"functionals/#reinitialize_weights","text":"reinitialize_weights() Re-initialize the weights and biases of a functional object. Arguments Returns","title":"reinitialize_weights"},{"location":"functionals/#count_params","text":"count_params() Total number of parameters of a functional. Arguments Returns Total number of parameters.","title":"count_params"},{"location":"functionals/#set_trainable","text":"set_trainable(val, layers=None) Set the weights and biases of a functional object trainable or not-trainable. Note: The SciModel should be called after this. Arguments val : (Ture, False) layers : list of layers to be set trainable or non-trainable. defaulted to None. Returns","title":"set_trainable"},{"location":"functionals/#split","text":"split() In the case of Functional with multiple outputs, you can split the outputs and get an associated functional. Returns (f1, f2, ...): Tuple of splitted Functional objects associated to each output.","title":"split"},{"location":"scimodels/","text":"Intro SciModel is similar to Keras' Model , prepared to make scientific model creation effortless. The inputs are of Variable objects, and the outputs are Target objects. As an example: from sciann import Variable, Functional, SciModel, Data x = Variable('x') y = Variable('y') Fxy = Functional('Fxy', [x, y], hidden_layers=[10, 20, 10], activation='tanh') model = SciModel([x,y], Data(Fxy)) SciModel can be trained by calling model.train . training_history = model.train([X_data, Y_data], Fxy_data) training_history object records the loss for each epoch as well as other parameters. Check Keras' documentation for more details. SciModel object also provides functionality such as predict and save . [source] SciModel sciann.models.model.SciModel(inputs=None, targets=None, loss_func='mse', optimizer='adam', load_weights_from=None, plot_to_file=None) Configures the model for training. Example: Arguments inputs : Main variables (also called inputs, or independent variables) of the network, xs . They all should be of type Variable . targets : list all targets (also called outputs, or dependent variables) to be satisfied during the training. Expected list members are: Entries of type Constraint , such as Data, Tie, etc. Entries of type Functional can be: . A single Functional : will be treated as a Data constraint. The object can be just a Functional or any derivatives of Functional s. An example is a PDE that is supposed to be zero. . A tuple of ( Functional , Functional ): will be treated as a Constraint of type Tie . If you need to impose more complex types of constraints or to impose a constraint partially in a specific part of region, use Data or Tie classes from Constraint . loss_func : defaulted to \"mse\" or \"mean_squared_error\". It can be an string from supported loss functions, i.e. (\"mse\" or \"mae\"). Alternatively, you can create your own loss function and pass the function handle (check Keras for more information). optimizer : defaulted to \"adam\" optimizer. It can be one of Keras accepted optimizers, e.g. \"adam\". You can also pass more details on the optimizer: optimizer = k.optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0) optimizer = k.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False) optimizer = k.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) Check our Keras documentation for further details. We have found load_weights_from : (file_path) Instantiate state of the model from a previously saved state. plot_to_file : A string file name to output the network architecture. Raises ValueError : inputs must be of type Variable. targets must be of types Functional , or ( Functional , data), or ( Functional , Functional ). train train(x_true, y_true, weights=None, target_weights=None, batch_size=64, epochs=100, learning_rate=0.001, adaptive_weights=False, log_adaptive_weights=None, log_loss_gradients=None, shuffle=True, callbacks=None, stop_lr_value=1e-08, reduce_lr_after=None, reduce_lr_min_delta=0.0, stop_after=None, stop_loss_value=1e-08, log_parameters=None, log_parameters_freq=None, save_weights_to=None, save_weights_freq=0, default_zero_weight=0.0, validation_data=None) Performs the training on the model. Arguments x_true : list of Xs associated to targets of Y . Expecting a list of np.ndarray of size (N,1) each, with N as the sample size. y_true : list of true Ys associated to the targets defined during model setup. Expecting the same size as list of targets defined in SciModel . To impose the targets at specific Xs only, pass a tuple of (ids, y_true) for that target. weights : (np.ndarray) A global sample weight to be applied to samples. Expecting an array of shape (N,1), with N as the sample size. Default value is one to consider all samples equally important. target_weights : (list) A weight for each target defined in y_true . batch_size : (Integer) or 'None'. Number of samples per gradient update. If unspecified, 'batch_size' will default to 2^6=64. epochs : (Integer) Number of epochs to train the model. Defaulted to 100. An epoch is an iteration over the entire x and y data provided. learning_rate : (Tuple/List) (epochs, lrs). Expects a list/tuple with a list of epochs and a list or learning rates. It linearly interpolates between entries. Defaulted to 0.001 with no decay. Example: learning_rate = ([0, 100, 1000], [0.001, 0.0005, 0.00001]) shuffle : Boolean (whether to shuffle the training data). Default value is True. adaptive_weights : Defaulted to False (no updates - evaluated once in the beginning). Used if the model is compiled with adaptive_weights. log_adaptive_weights : Logging the weights and gradients of adaptive_weight. Defaulted to adaptive_weights. log_loss_gradients : Frequency for logging the norm2 of gradients of each target. Defaulted to None. callbacks : List of keras.callbacks.Callback instances. reduce_lr_after : patience to reduce learning rate or stop after certain missed epochs. Defaulted to epochs max(10, epochs/10). stop_lr_value : stop the training if learning rate goes lower than this value. Defaulted to 1e-8. reduce_lr_min_delta : min absolute change in total loss value that is considered a successful change. Defaulted to 0.001. This values affects number of failed attempts to trigger reduce learning rate based on reduce_lr_after. stop_after : To stop after certain missed epochs. Defaulted to total number of epochs. stop_loss_value : The minimum value of the total loss that stops the training automatically. Defaulted to 1e-8. save_weights_to : (file_path) If you want to save the state of the model (at the end of the training). save_weights_freq : (Integer) Save weights every N epcohs. Defaulted to 0. default_zero_weight : a small number for zero sample-weight. Returns A Keras 'History' object after performing fitting. predict predict(xs, batch_size=None, verbose=0, steps=None) Predict output from network. Arguments xs : list of Xs associated model. Expecting a list of np.ndarray of size (N,1) each, with N as the sample size. batch_size : defaulted to None. Check Keras documentation for more information. verbose : defaulted to 0 (None). Check Keras documentation for more information. steps : defaulted to 0 (None). Check Keras documentation for more information. Returns List of numpy array of the size of network outputs. Raises ValueError if number of xs s is different from number of inputs . loss_functions loss_function) loss_function returns the callable object to evaluate the loss. Arguments method : String. \"mse\" for Mean Squared Error or \"mae\" for Mean Absolute Error or \"se\" for Squared Error or \"ae\" for Absolute Error . Returns Callable function that gets (y_true, y_pred) as the input and returns the loss value as the output. Raises ValueError if anything other than \"mse\" or \"mae\" is passed.","title":"SciModels"},{"location":"scimodels/#intro","text":"SciModel is similar to Keras' Model , prepared to make scientific model creation effortless. The inputs are of Variable objects, and the outputs are Target objects. As an example: from sciann import Variable, Functional, SciModel, Data x = Variable('x') y = Variable('y') Fxy = Functional('Fxy', [x, y], hidden_layers=[10, 20, 10], activation='tanh') model = SciModel([x,y], Data(Fxy)) SciModel can be trained by calling model.train . training_history = model.train([X_data, Y_data], Fxy_data) training_history object records the loss for each epoch as well as other parameters. Check Keras' documentation for more details. SciModel object also provides functionality such as predict and save . [source]","title":"Intro"},{"location":"scimodels/#scimodel","text":"sciann.models.model.SciModel(inputs=None, targets=None, loss_func='mse', optimizer='adam', load_weights_from=None, plot_to_file=None) Configures the model for training. Example: Arguments inputs : Main variables (also called inputs, or independent variables) of the network, xs . They all should be of type Variable . targets : list all targets (also called outputs, or dependent variables) to be satisfied during the training. Expected list members are: Entries of type Constraint , such as Data, Tie, etc. Entries of type Functional can be: . A single Functional : will be treated as a Data constraint. The object can be just a Functional or any derivatives of Functional s. An example is a PDE that is supposed to be zero. . A tuple of ( Functional , Functional ): will be treated as a Constraint of type Tie . If you need to impose more complex types of constraints or to impose a constraint partially in a specific part of region, use Data or Tie classes from Constraint . loss_func : defaulted to \"mse\" or \"mean_squared_error\". It can be an string from supported loss functions, i.e. (\"mse\" or \"mae\"). Alternatively, you can create your own loss function and pass the function handle (check Keras for more information). optimizer : defaulted to \"adam\" optimizer. It can be one of Keras accepted optimizers, e.g. \"adam\". You can also pass more details on the optimizer: optimizer = k.optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0) optimizer = k.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False) optimizer = k.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) Check our Keras documentation for further details. We have found load_weights_from : (file_path) Instantiate state of the model from a previously saved state. plot_to_file : A string file name to output the network architecture. Raises ValueError : inputs must be of type Variable. targets must be of types Functional , or ( Functional , data), or ( Functional , Functional ).","title":"SciModel"},{"location":"scimodels/#train","text":"train(x_true, y_true, weights=None, target_weights=None, batch_size=64, epochs=100, learning_rate=0.001, adaptive_weights=False, log_adaptive_weights=None, log_loss_gradients=None, shuffle=True, callbacks=None, stop_lr_value=1e-08, reduce_lr_after=None, reduce_lr_min_delta=0.0, stop_after=None, stop_loss_value=1e-08, log_parameters=None, log_parameters_freq=None, save_weights_to=None, save_weights_freq=0, default_zero_weight=0.0, validation_data=None) Performs the training on the model. Arguments x_true : list of Xs associated to targets of Y . Expecting a list of np.ndarray of size (N,1) each, with N as the sample size. y_true : list of true Ys associated to the targets defined during model setup. Expecting the same size as list of targets defined in SciModel . To impose the targets at specific Xs only, pass a tuple of (ids, y_true) for that target. weights : (np.ndarray) A global sample weight to be applied to samples. Expecting an array of shape (N,1), with N as the sample size. Default value is one to consider all samples equally important. target_weights : (list) A weight for each target defined in y_true . batch_size : (Integer) or 'None'. Number of samples per gradient update. If unspecified, 'batch_size' will default to 2^6=64. epochs : (Integer) Number of epochs to train the model. Defaulted to 100. An epoch is an iteration over the entire x and y data provided. learning_rate : (Tuple/List) (epochs, lrs). Expects a list/tuple with a list of epochs and a list or learning rates. It linearly interpolates between entries. Defaulted to 0.001 with no decay. Example: learning_rate = ([0, 100, 1000], [0.001, 0.0005, 0.00001]) shuffle : Boolean (whether to shuffle the training data). Default value is True. adaptive_weights : Defaulted to False (no updates - evaluated once in the beginning). Used if the model is compiled with adaptive_weights. log_adaptive_weights : Logging the weights and gradients of adaptive_weight. Defaulted to adaptive_weights. log_loss_gradients : Frequency for logging the norm2 of gradients of each target. Defaulted to None. callbacks : List of keras.callbacks.Callback instances. reduce_lr_after : patience to reduce learning rate or stop after certain missed epochs. Defaulted to epochs max(10, epochs/10). stop_lr_value : stop the training if learning rate goes lower than this value. Defaulted to 1e-8. reduce_lr_min_delta : min absolute change in total loss value that is considered a successful change. Defaulted to 0.001. This values affects number of failed attempts to trigger reduce learning rate based on reduce_lr_after. stop_after : To stop after certain missed epochs. Defaulted to total number of epochs. stop_loss_value : The minimum value of the total loss that stops the training automatically. Defaulted to 1e-8. save_weights_to : (file_path) If you want to save the state of the model (at the end of the training). save_weights_freq : (Integer) Save weights every N epcohs. Defaulted to 0. default_zero_weight : a small number for zero sample-weight. Returns A Keras 'History' object after performing fitting.","title":"train"},{"location":"scimodels/#predict","text":"predict(xs, batch_size=None, verbose=0, steps=None) Predict output from network. Arguments xs : list of Xs associated model. Expecting a list of np.ndarray of size (N,1) each, with N as the sample size. batch_size : defaulted to None. Check Keras documentation for more information. verbose : defaulted to 0 (None). Check Keras documentation for more information. steps : defaulted to 0 (None). Check Keras documentation for more information. Returns List of numpy array of the size of network outputs. Raises ValueError if number of xs s is different from number of inputs .","title":"predict"},{"location":"scimodels/#loss_functions","text":"loss_function) loss_function returns the callable object to evaluate the loss. Arguments method : String. \"mse\" for Mean Squared Error or \"mae\" for Mean Absolute Error or \"se\" for Squared Error or \"ae\" for Absolute Error . Returns Callable function that gets (y_true, y_pred) as the input and returns the loss value as the output. Raises ValueError if anything other than \"mse\" or \"mae\" is passed.","title":"loss_functions"},{"location":"utils/","text":"Intro grad sciann.utils.grad() Computes gradient tensor of functional object f. Arguments f : Functional object. ys : layer name for ys to differentiate. xs : layer name for xs to be differentiated w.r.t. order : order of differentiation w.r.t. xs - defaulted to 1. Returns A new functional object. diag_grad sciann.utils.diag_grad() Computes diag of gradient tensor of functional object f. Arguments f : Functional object. ys : layer name for ys to differentiate. xs : layer name for xs to be differentiated w.r.t. order : order of differentiation w.r.t. xs - defaulted to 1. Returns A new functional object. div sciann.utils.div(other) Element-wise division applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional. radial_basis sciann.utils.radial_basis(ci, radii) Apply radial_basis function to x element-wise. Arguments xs : List of functional objects. ci : Center of basis functional (same length as xs). radii : standard deviation or radius from the center. Returns A new functional object. sin sciann.utils.sin() Computes sin of x element-wise. Arguments x : Functional object. Returns A new functional object. asin sciann.utils.asin() Computes asin of x element-wise. Arguments x : Functional object. Returns A new functional object. cos sciann.utils.cos() Computes cos of x element-wise. Arguments x : Functional object. Returns A new functional object. acos sciann.utils.acos() Computes acos of x element-wise. Arguments x : Functional object. Returns A new functional object. tan sciann.utils.tan() Computes tan of x element-wise. Arguments x : Functional object. Returns A new functional object. atan sciann.utils.atan() Computes atan of x element-wise. Arguments x : Functional object. Returns A new functional object. tanh sciann.utils.tanh() Computes tanh of x element-wise. Arguments x : Functional object. Returns A new functional object. exp sciann.utils.exp() Computes exp of x element-wise. Arguments x : Functional object. Returns A new functional object. pow sciann.utils.pow(b) Element-wise exponentiation applied to the Functional object. Arguments a, b: pow(a,b) Note that at least one of them should be of type Functional. Returns A Functional. add sciann.utils.add(other) Element-wise addition applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional. sub sciann.utils.sub(other) Element-wise subtraction applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional. mul sciann.utils.mul(other) Element-wise multiplication applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional. div sciann.utils.div(other) Element-wise division applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"Utils"},{"location":"utils/#intro","text":"","title":"Intro"},{"location":"utils/#grad","text":"sciann.utils.grad() Computes gradient tensor of functional object f. Arguments f : Functional object. ys : layer name for ys to differentiate. xs : layer name for xs to be differentiated w.r.t. order : order of differentiation w.r.t. xs - defaulted to 1. Returns A new functional object.","title":"grad"},{"location":"utils/#diag_grad","text":"sciann.utils.diag_grad() Computes diag of gradient tensor of functional object f. Arguments f : Functional object. ys : layer name for ys to differentiate. xs : layer name for xs to be differentiated w.r.t. order : order of differentiation w.r.t. xs - defaulted to 1. Returns A new functional object.","title":"diag_grad"},{"location":"utils/#div","text":"sciann.utils.div(other) Element-wise division applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"div"},{"location":"utils/#radial_basis","text":"sciann.utils.radial_basis(ci, radii) Apply radial_basis function to x element-wise. Arguments xs : List of functional objects. ci : Center of basis functional (same length as xs). radii : standard deviation or radius from the center. Returns A new functional object.","title":"radial_basis"},{"location":"utils/#sin","text":"sciann.utils.sin() Computes sin of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"sin"},{"location":"utils/#asin","text":"sciann.utils.asin() Computes asin of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"asin"},{"location":"utils/#cos","text":"sciann.utils.cos() Computes cos of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"cos"},{"location":"utils/#acos","text":"sciann.utils.acos() Computes acos of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"acos"},{"location":"utils/#tan","text":"sciann.utils.tan() Computes tan of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"tan"},{"location":"utils/#atan","text":"sciann.utils.atan() Computes atan of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"atan"},{"location":"utils/#tanh","text":"sciann.utils.tanh() Computes tanh of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"tanh"},{"location":"utils/#exp","text":"sciann.utils.exp() Computes exp of x element-wise. Arguments x : Functional object. Returns A new functional object.","title":"exp"},{"location":"utils/#pow","text":"sciann.utils.pow(b) Element-wise exponentiation applied to the Functional object. Arguments a, b: pow(a,b) Note that at least one of them should be of type Functional. Returns A Functional.","title":"pow"},{"location":"utils/#add","text":"sciann.utils.add(other) Element-wise addition applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"add"},{"location":"utils/#sub","text":"sciann.utils.sub(other) Element-wise subtraction applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"sub"},{"location":"utils/#mul","text":"sciann.utils.mul(other) Element-wise multiplication applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"mul"},{"location":"utils/#div_1","text":"sciann.utils.div(other) Element-wise division applied to the Functional objects. Arguments f : Functional object. other : A python number or a tensor or a functional object. Returns A Functional.","title":"div"},{"location":"variables/","text":"Intro Variable is a way to to define inputs to the network, very much similar to the Input class in Keras . However, since we need to perform differentiation and other operations on the network, we cannot just use Input . Instead, we need to define the inputs of the network through Variable . For scientific computations, a Variable has only a dimension of 1. Therefore, if you need to have a three-dimensional coordinate inputs, you need to define three variables: from sciann import Variable x = Variable('x') y = Variable('y') z = Variable('z') This is precisely because we need to perform differentiation with respect to (x, y, z). [source] Variable sciann.functionals.variable.Variable(name=None, units=1, tensor=None, dtype=None) Configures the Variable object for the network's input. Arguments name : String. Required as derivatives work only with layer names. units : Int. Number of feature of input var. tensor : Tensorflow Tensor . Can be pass as the input path. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises","title":"Variables"},{"location":"variables/#intro","text":"Variable is a way to to define inputs to the network, very much similar to the Input class in Keras . However, since we need to perform differentiation and other operations on the network, we cannot just use Input . Instead, we need to define the inputs of the network through Variable . For scientific computations, a Variable has only a dimension of 1. Therefore, if you need to have a three-dimensional coordinate inputs, you need to define three variables: from sciann import Variable x = Variable('x') y = Variable('y') z = Variable('z') This is precisely because we need to perform differentiation with respect to (x, y, z). [source]","title":"Intro"},{"location":"variables/#variable","text":"sciann.functionals.variable.Variable(name=None, units=1, tensor=None, dtype=None) Configures the Variable object for the network's input. Arguments name : String. Required as derivatives work only with layer names. units : Int. Number of feature of input var. tensor : Tensorflow Tensor . Can be pass as the input path. dtype : data-type of the network parameters, can be ('float16', 'float32', 'float64'). Raises","title":"Variable"},{"location":"why-use-sciann/","text":"Why use SciANN among all other codes? The main purpose of SciANN is a platform for people with Scientific Computations backgrounds in mind. You will find this code very useful for: Solving ODEs and PDEs using densely connect, complex networks, recurrent networks are on the way. This platform is ready to use for Curve Fitting, Differentiations, Integration, etc. If you have other scientific computations in mind that are not implemented yet, contact us . As an example, let's fit a neural network with three-hidden layers, each with 10 neurons and \\( \\tanh \\) activation function, on data generated from \\( sin(x) \\): import numpy as np from sciann import Variable, Functional, SciModel from sciann.constraints import Data # Synthetic data generated from sin function over [0, 2pi] x_true = np.linspace(0, np.pi*2, 10000) y_true = np.sin(x_true) # The network inputs should be defined with Variable. x = Variable('x', dtype='float32') # Each network is defined by Functional. y = Functional('y', x, [10, 10, 10], activation='tanh') # The training data is a condition (constraint) on the model. c1 = Data(y) # The model is formed with input `x` and condition `c1`. model = SciModel(x, c1) # Training: .solve runs the optimization and finds the parameters. model.train(x_true, y_true, batch_size=32, epochs=100) # used to evaluate the model after the training. y_pred = model.predict(x_true) As you may find, this code takes advantage of Keras great design and takes it to the next level for scientific computations.","title":"Why use SciANN"},{"location":"why-use-sciann/#why-use-sciann-among-all-other-codes","text":"The main purpose of SciANN is a platform for people with Scientific Computations backgrounds in mind. You will find this code very useful for: Solving ODEs and PDEs using densely connect, complex networks, recurrent networks are on the way. This platform is ready to use for Curve Fitting, Differentiations, Integration, etc. If you have other scientific computations in mind that are not implemented yet, contact us . As an example, let's fit a neural network with three-hidden layers, each with 10 neurons and \\( \\tanh \\) activation function, on data generated from \\( sin(x) \\): import numpy as np from sciann import Variable, Functional, SciModel from sciann.constraints import Data # Synthetic data generated from sin function over [0, 2pi] x_true = np.linspace(0, np.pi*2, 10000) y_true = np.sin(x_true) # The network inputs should be defined with Variable. x = Variable('x', dtype='float32') # Each network is defined by Functional. y = Functional('y', x, [10, 10, 10], activation='tanh') # The training data is a condition (constraint) on the model. c1 = Data(y) # The model is formed with input `x` and condition `c1`. model = SciModel(x, c1) # Training: .solve runs the optimization and finds the parameters. model.train(x_true, y_true, batch_size=32, epochs=100) # used to evaluate the model after the training. y_pred = model.predict(x_true) As you may find, this code takes advantage of Keras great design and takes it to the next level for scientific computations.","title":"Why use SciANN among all other codes?"},{"location":"examples/curve-fitting-1d/","text":"Curve fitting in 1D {{autogenerated}}","title":"Curve fitting in 1D"},{"location":"examples/curve-fitting-1d/#curve-fitting-in-1d","text":"{{autogenerated}}","title":"Curve fitting in 1D"},{"location":"examples/example-fitting-1d-lstm/","text":"Curve fitting in 1D Here, a 1D curve fitting example is explored. Imagine, a synthetic data generated from \\( \\sin(x) \\) over the range of \\( [0, 2\\pi] \\). To train a neural network model on this curve, you should first define a Variable . A neural network with three layers, each containing 10 neurons, and with tanh activation function is then generated using the Functional class. The target is imposed on the output using the Data class from Constraint , and passed to the SciModel to form a Sciann model. import numpy as np from sciann.functionals.rnn_variable import RNNVariable from sciann.functionals.rnn_functional import RNNFunctional from sciann.functionals.rnn_field import RNNField from sciann import SciModel from sciann.utils import diff, set_random_seed from sciann.constraints import Data, Tie set_random_seed(1234) tunits = 3 # Synthetic data generated from sin function over [0, 2pi] # x_true = np.linspace(0, np.pi*2, 10000).reshape(-1, tunits, 1) x_true = np.linspace(0, np.pi*2, 100).reshape(-1, 1) dx = np.diff(x_true.flatten()).mean() x_true = x_true + np.linspace(0, dx*(tunits-1), tunits).reshape(1, -1) y_true = np.sin(x_true) dy_true = np.cos(x_true) #.sum(axis=1, keepdims=True)*2.0 # The network inputs should be defined with Variable. t = RNNVariable(tunits, name='t', dtype='float64') # Each network is defined by Functional. y = RNNFunctional( 'y', t, [5], activation='tanh', recurrent_activation='linear', kernel_initializer=1.0, recurrent_initializer=1.0, bias_initializer=1.0, rnn_type=\"SimpleRNN\" ) dy= diff(y, t) def rnn_cell(x): y = [] actf = lambda x: x for i, xi in enumerate(x): if i==0: y.append( actf(np.ones([1,5]) + xi*np.ones([1,5])) ) else: y.append( actf(np.ones([1, 5]) + xi * np.ones([1, 5]) + np.matmul(y[-1], np.ones([5, 5]))) ) actf = lambda x: np.tanh(x) y = [actf(yi) for yi in y] return y def rnn_net(x): actf = lambda x: x ys = rnn_cell(x) return np.concatenate([1 + np.matmul(actf(y), np.ones([5,1])) for y in ys], axis=-1).flatten() test_rnn = lambda i: y.eval(x_true[i,:]) - rnn_net(x_true[i, :]) for i in range(10): print(test_rnn(i)) raise ValueError # Define the target (output) of your model. c1 = Data(y) # c2 = Data((y[1]-y[0])/dx) c2 = Data(dy) # The model is formed with input `x` and condition `c1`. model = SciModel(t, [c1, c2]) # Training: .train runs the optimization and finds the parameters. model.train( x_true.reshape(-1, tunits, 1), [y_true.reshape(-1, tunits, 1), dy_true.reshape(-1, tunits, 1)], batch_size=1000000, epochs=20000, learning_rate=0.01 ) # used to evaluate the model after the training. # x_pred = np.linspace(0, np.pi*4, 20000).reshape(-1, tunits, 1) x_pred = np.linspace(0, np.pi*4, 200).reshape(-1, 1) dx = np.diff(x_pred.flatten()).mean() x_pred = x_pred + np.linspace(0, dx*(tunits-1), tunits).reshape(1, -1) y_pred = y.eval(model, x_pred) # dy_pred = dy.eval(model, x_pred) y_star = np.sin(x_pred) dy_star = np.cos(x_pred) import matplotlib.pyplot as plt plt.plot(x_pred[:,0], y_pred[:,0], x_pred[:,0], y_star[:,0]) # plt.plot(x_pred, dy_pred.reshape(-1), x_pred, dy_star) plt.show()","title":"Curve fitting in 1D"},{"location":"examples/example-fitting-1d-lstm/#curve-fitting-in-1d","text":"Here, a 1D curve fitting example is explored. Imagine, a synthetic data generated from \\( \\sin(x) \\) over the range of \\( [0, 2\\pi] \\). To train a neural network model on this curve, you should first define a Variable . A neural network with three layers, each containing 10 neurons, and with tanh activation function is then generated using the Functional class. The target is imposed on the output using the Data class from Constraint , and passed to the SciModel to form a Sciann model. import numpy as np from sciann.functionals.rnn_variable import RNNVariable from sciann.functionals.rnn_functional import RNNFunctional from sciann.functionals.rnn_field import RNNField from sciann import SciModel from sciann.utils import diff, set_random_seed from sciann.constraints import Data, Tie set_random_seed(1234) tunits = 3 # Synthetic data generated from sin function over [0, 2pi] # x_true = np.linspace(0, np.pi*2, 10000).reshape(-1, tunits, 1) x_true = np.linspace(0, np.pi*2, 100).reshape(-1, 1) dx = np.diff(x_true.flatten()).mean() x_true = x_true + np.linspace(0, dx*(tunits-1), tunits).reshape(1, -1) y_true = np.sin(x_true) dy_true = np.cos(x_true) #.sum(axis=1, keepdims=True)*2.0 # The network inputs should be defined with Variable. t = RNNVariable(tunits, name='t', dtype='float64') # Each network is defined by Functional. y = RNNFunctional( 'y', t, [5], activation='tanh', recurrent_activation='linear', kernel_initializer=1.0, recurrent_initializer=1.0, bias_initializer=1.0, rnn_type=\"SimpleRNN\" ) dy= diff(y, t) def rnn_cell(x): y = [] actf = lambda x: x for i, xi in enumerate(x): if i==0: y.append( actf(np.ones([1,5]) + xi*np.ones([1,5])) ) else: y.append( actf(np.ones([1, 5]) + xi * np.ones([1, 5]) + np.matmul(y[-1], np.ones([5, 5]))) ) actf = lambda x: np.tanh(x) y = [actf(yi) for yi in y] return y def rnn_net(x): actf = lambda x: x ys = rnn_cell(x) return np.concatenate([1 + np.matmul(actf(y), np.ones([5,1])) for y in ys], axis=-1).flatten() test_rnn = lambda i: y.eval(x_true[i,:]) - rnn_net(x_true[i, :]) for i in range(10): print(test_rnn(i)) raise ValueError # Define the target (output) of your model. c1 = Data(y) # c2 = Data((y[1]-y[0])/dx) c2 = Data(dy) # The model is formed with input `x` and condition `c1`. model = SciModel(t, [c1, c2]) # Training: .train runs the optimization and finds the parameters. model.train( x_true.reshape(-1, tunits, 1), [y_true.reshape(-1, tunits, 1), dy_true.reshape(-1, tunits, 1)], batch_size=1000000, epochs=20000, learning_rate=0.01 ) # used to evaluate the model after the training. # x_pred = np.linspace(0, np.pi*4, 20000).reshape(-1, tunits, 1) x_pred = np.linspace(0, np.pi*4, 200).reshape(-1, 1) dx = np.diff(x_pred.flatten()).mean() x_pred = x_pred + np.linspace(0, dx*(tunits-1), tunits).reshape(1, -1) y_pred = y.eval(model, x_pred) # dy_pred = dy.eval(model, x_pred) y_star = np.sin(x_pred) dy_star = np.cos(x_pred) import matplotlib.pyplot as plt plt.plot(x_pred[:,0], y_pred[:,0], x_pred[:,0], y_star[:,0]) # plt.plot(x_pred, dy_pred.reshape(-1), x_pred, dy_star) plt.show()","title":"Curve fitting in 1D"},{"location":"examples/example-fitting-1d-rnn/","text":"import matplotlib.pyplot as plt import sciann as sn omega = 2*np.pi/2.0 omega_bar = 2*np.pi/1.5 A = 1.0 beta = 1.0 tunits = 2 NDATA = 1000 t_data = np.linspace(0, 4*np.pi, NDATA).reshape(-1,1) dt = np.diff(t_data.flatten()).mean() t_data = t_data + np.linspace(0, dt*(tunits-1), tunits).reshape(1, -1) y_data = A*(np.sin(omega*t_data) - beta*np.sin(omega_bar*t_data)) # Add noise # y_noise = 0.15*np.std(y_data)*np.random.randn(NDATA) t = sn.functionals.RNNVariable(tunits, 't') y = sn.functionals.RNNFunctional('y', t, [1], 'sin', recurrent_activation='sin') mRNN = sn.SciModel(t, y) mRNN.train(t_data, y_data, learning_rate=0.001, epochs=10000, batch_size=100)","title":"Example fitting 1d rnn"},{"location":"examples/example-fitting-1d/","text":"Curve fitting in 1D Here, a 1D curve fitting example is explored. Imagine, a synthetic data generated from \\( \\sin(x) \\) over the range of \\( [0, 2\\pi] \\). To train a neural network model on this curve, you should first define a Variable . A neural network with three layers, each containing 10 neurons, and with tanh activation function is then generated using the Functional class. The target is imposed on the output using the Data class from Constraint , and passed to the SciModel to form a SciANN model. import numpy as np from sciann import Variable, Functional, SciModel, Parameter from sciann.constraints import Data, MinMax from sciann.utils.math import diff import sciann as sn sn.set_random_seed(1234) # Synthetic data generated from sin function over [0, 2pi] x_true = np.linspace(0, np.pi*2, 10000) y_true = np.sin(x_true) # The network inputs should be defined with Variable. x = Variable('x', dtype='float64') # Each network is defined by Functional. y = Functional('y', x, [10, 10, 10], activation='l-tanh', res_net=True) d = Parameter(10.0, inputs=x, name='d') # Define the target (output) of your model. c1 = Data(y) L = d*diff(y, x, order=2) + y # The model is formed with input `x` and condition `c1`. model = SciModel(x, [c1, sn.PDE(L)]) # Training: .train runs the optimization and finds the parameters. history = model.train( x_true, [y_true, 'zeros'], batch_size=32, epochs=100, adaptive_weights=True, log_parameters=[d] ) # used to evaluate the model after the training. y_pred = y.eval(model, x_true) d_pred = d.eval(model, x_true)","title":"Curve fitting in 1D"},{"location":"examples/example-fitting-1d/#curve-fitting-in-1d","text":"Here, a 1D curve fitting example is explored. Imagine, a synthetic data generated from \\( \\sin(x) \\) over the range of \\( [0, 2\\pi] \\). To train a neural network model on this curve, you should first define a Variable . A neural network with three layers, each containing 10 neurons, and with tanh activation function is then generated using the Functional class. The target is imposed on the output using the Data class from Constraint , and passed to the SciModel to form a SciANN model. import numpy as np from sciann import Variable, Functional, SciModel, Parameter from sciann.constraints import Data, MinMax from sciann.utils.math import diff import sciann as sn sn.set_random_seed(1234) # Synthetic data generated from sin function over [0, 2pi] x_true = np.linspace(0, np.pi*2, 10000) y_true = np.sin(x_true) # The network inputs should be defined with Variable. x = Variable('x', dtype='float64') # Each network is defined by Functional. y = Functional('y', x, [10, 10, 10], activation='l-tanh', res_net=True) d = Parameter(10.0, inputs=x, name='d') # Define the target (output) of your model. c1 = Data(y) L = d*diff(y, x, order=2) + y # The model is formed with input `x` and condition `c1`. model = SciModel(x, [c1, sn.PDE(L)]) # Training: .train runs the optimization and finds the parameters. history = model.train( x_true, [y_true, 'zeros'], batch_size=32, epochs=100, adaptive_weights=True, log_parameters=[d] ) # used to evaluate the model after the training. y_pred = y.eval(model, x_true) d_pred = d.eval(model, x_true)","title":"Curve fitting in 1D"},{"location":"examples/test_diff/","text":"from sciann import Variable, Field, Functional, SciModel from sciann.utils.math import diag_grad, div_grad, grad, dot from sciann.utils.math import diag_part, diag import keras.backend as K num_node = 1000 famil = np.arange(0, num_node).reshape(-1, 1) + np.array([-1, 0, 1]) famil[famil>999] = num_node - famil[famil>999] xtrain = np.linspace(-2*np.pi, 2*np.pi, num_node) gf00 = np.concatenate([np.ones((num_node,1))*0.2, np.ones((num_node,1))*0.6, np.ones((num_node,1))*0.2], axis=-1) x = Variable(\"x\", units=3, dtype='float64') GF00 = Variable(\"GF00\", units=3, dtype='float64') y = Field(\"y\", units=3, dtype='float64') fx = Functional(y, x, 4*[10], 'tanh') yf = dot(fx, GF00) gf = grad(fx, x) gf1 = diag_grad(fx, x) gf2 = div_grad(fx, x) dotgf = dot(gf, diag(GF00)) raise ValueError model = SciModel([x, GF00], gf) model.train([xtrain[famil], gf00], np.cos(xtrain), epochs=1000) xtest = np.linspace(-4*np.pi, 4*np.pi, num_node) ypred = yf.eval(model, [xtest[famil], gf00]) dypred = gf.eval(model, [xtest[famil], gf00]) import matplotlib.pyplot as plt plt.plot(xtest, ypred, 'b', xtest, np.sin(xtest), '--g') plt.plot(xtest, dypred, 'r', xtest, np.cos(xtest), '--k') plt.show()","title":"Test diff"},{"location":"getting-started/functional-guide/","text":"Using Functional to form complex network architectures The Functional class is designed to allow users to design complex networks with a few lines of code. To use Functional, you can follow the exmaple bellow: import numpy as np from sciann import Variable, Functional, SciModel from sciann.constraints import Data from sciann.utils import sin, cos, sinh # Synthetic data to be fitted. x_true = np.linspace(0.0, 2*np.pi, 10000) y_true = np.sin(x_true) # Functional requires input features to be defined through Variable. x = Variable(\"x\", dtype='float32') # A complex network with 5 hidden layers ([5, 10, 20, 10, 5]), # and feature aumentation [x, x**2, x**3, sin(x), cos(x), sinh(x)]. y = Functional( \"y\", [x, x**2, x**3, sin(x), cos(x), sinh(x)], hidden_layers = [5, 10, 20, 10, 5], activations = 'tanh', ) # Define the SciModel. model = SciModel(x, Data(y)) # Solve the neural network model. model.solve(x_true, y_true, epochs=32, batches=10) # Find model's prediciton. y_pred = model.predict(x_true) Alternatively, you can also evaluate each individual variable after training: y_pred = y.eval(model, x_true)","title":"Guide to Functional"},{"location":"getting-started/functional-guide/#using-functional-to-form-complex-network-architectures","text":"The Functional class is designed to allow users to design complex networks with a few lines of code. To use Functional, you can follow the exmaple bellow: import numpy as np from sciann import Variable, Functional, SciModel from sciann.constraints import Data from sciann.utils import sin, cos, sinh # Synthetic data to be fitted. x_true = np.linspace(0.0, 2*np.pi, 10000) y_true = np.sin(x_true) # Functional requires input features to be defined through Variable. x = Variable(\"x\", dtype='float32') # A complex network with 5 hidden layers ([5, 10, 20, 10, 5]), # and feature aumentation [x, x**2, x**3, sin(x), cos(x), sinh(x)]. y = Functional( \"y\", [x, x**2, x**3, sin(x), cos(x), sinh(x)], hidden_layers = [5, 10, 20, 10, 5], activations = 'tanh', ) # Define the SciModel. model = SciModel(x, Data(y)) # Solve the neural network model. model.solve(x_true, y_true, epochs=32, batches=10) # Find model's prediciton. y_pred = model.predict(x_true) Alternatively, you can also evaluate each individual variable after training: y_pred = y.eval(model, x_true)","title":"Using Functional to form complex network architectures"},{"location":"getting-started/scimodel-guide/","text":"Getting started with the SciANN model or SciModel The SciModel is the relation between network inputs, i.e. Variable and network outputs, i.e. Conditions . You can set up a SciModel as simple as the code bellow: from sciann import Variable, Functional, SciModel from sciann.constraints import Data x = Variable(\"x\") y = Functional(\"y\", x) cy = Data(y) model = SciModel(cy)","title":"Guide to SciANN model"},{"location":"getting-started/scimodel-guide/#getting-started-with-the-sciann-model-or-scimodel","text":"The SciModel is the relation between network inputs, i.e. Variable and network outputs, i.e. Conditions . You can set up a SciModel as simple as the code bellow: from sciann import Variable, Functional, SciModel from sciann.constraints import Data x = Variable(\"x\") y = Functional(\"y\", x) cy = Data(y) model = SciModel(cy)","title":"Getting started with the SciANN model or SciModel"}]}